---
title: "Review: ImageNet Classification with Deep Convolutional Neural Networks"
author: "Paidamoyo Chapfuwa"
date: "01/20/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Explore Non-Linearities
The activation functions explored:relu, tanh, abs(tanh) and sigmoid. As expected relu takes shorter time to achieve the same level of accuracy on MNIT data.

* abs(tanh) - Time usage: 0:06:41, Accuracy on Test-Set: 98.7% (9867 / 10000)
* relu - Time usage: 0:06:32, Accuracy on Test-Set: 98.7% (9866 / 10000)
* tanh - Time usage: 0:06:37, Accuracy on Test-Set: 98.9% (9887 / 10000)
* sigmoid - Time usage: 0:06:30, Accuracy on Test-Set: 96.5% (9652 / 10000)
```{r}
x <- seq(from = -40, to = 40, by = 0.01)

sig <- 1 / (1 + exp(-x))
hinge <- function(a) {
  return(max(0, a))
}
relu <- unlist(lapply(x, hinge))
softplus <- log(1 + exp(x))

plot(x,
     relu,
     type = 'l',
     ylab = '')
lines(x, sig , col = 'red')
lines(x, tanh(x), col = 'green')
lines(x, abs(tanh(x)), col = 'blue')
lines(x, softplus, col = 'purple')
legend(0, 0, legend=c("relu", "sigmoid", "tanh", "abs(tanh)", "softplus"), lty = c(1,1,1,1, 1), col = c('black', "red", 'green', 'blue', "purple") )



```


